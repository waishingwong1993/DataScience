{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords         \n",
    "from nltk.stem import PorterStemmer       \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import heapq\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "# sk-learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def preprocess(sentence):\n",
    "    '''\n",
    "    Preprocesses the data\n",
    "    '''\n",
    "\n",
    "    # Tokenize & lower casing\n",
    "    sentence = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Stemmer & Stop words\n",
    "    sentence = [stemmer.stem(i) for i in sentence if (i not in stopwords_english) and (i not in string.punctuation)]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df['original_text'] = df['text'].copy()\n",
    "df['text'] = df['text'].apply(lambda x:preprocess(x))\n",
    "\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "# split the data into 2 sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'conclus\",\n",
       " \"'i\",\n",
       " \"'ll\",\n",
       " \"'re\",\n",
       " \"'the\",\n",
       " \"'we\",\n",
       " '..',\n",
       " '15',\n",
       " '40',\n",
       " '50',\n",
       " '60',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'abc',\n",
       " 'ablaz',\n",
       " 'accid',\n",
       " 'account',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'actual',\n",
       " 'affect',\n",
       " 'ago',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'airplan',\n",
       " 'airport',\n",
       " 'alarm',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alreadi',\n",
       " 'also',\n",
       " 'alway',\n",
       " 'ambul',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amid',\n",
       " 'amp',\n",
       " 'angri',\n",
       " 'annihil',\n",
       " 'anniversari',\n",
       " 'anoth',\n",
       " 'answer',\n",
       " 'anthrax',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'apocalyps',\n",
       " 'appear',\n",
       " 'area',\n",
       " 'armageddon',\n",
       " 'armi',\n",
       " 'around',\n",
       " 'arson',\n",
       " 'ass',\n",
       " 'atom',\n",
       " 'attack',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'avalanch',\n",
       " 'away',\n",
       " 'b',\n",
       " 'babi',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'ball',\n",
       " 'ban',\n",
       " 'bang',\n",
       " 'bar',\n",
       " 'battl',\n",
       " 'bc',\n",
       " 'beach',\n",
       " 'beauti',\n",
       " 'becom',\n",
       " 'begin',\n",
       " 'behind',\n",
       " 'believ',\n",
       " 'best',\n",
       " 'bestnaijamad',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'bioterror',\n",
       " 'black',\n",
       " 'blast',\n",
       " 'blaze',\n",
       " 'bleed',\n",
       " 'blew',\n",
       " 'blight',\n",
       " 'blizzard',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'bloodi',\n",
       " 'blown',\n",
       " 'boat',\n",
       " 'bodi',\n",
       " 'bomb',\n",
       " 'bomber',\n",
       " 'book',\n",
       " 'boy',\n",
       " 'break',\n",
       " 'bridg',\n",
       " 'bring',\n",
       " 'brown',\n",
       " 'bu',\n",
       " 'build',\n",
       " 'burn',\n",
       " 'bush',\n",
       " 'busi',\n",
       " 'ca',\n",
       " 'cake',\n",
       " 'calgari',\n",
       " 'california',\n",
       " 'call',\n",
       " 'came',\n",
       " 'camp',\n",
       " 'captur',\n",
       " 'car',\n",
       " 'care',\n",
       " 'career',\n",
       " 'case',\n",
       " 'casualti',\n",
       " 'cat',\n",
       " 'catastroph',\n",
       " 'caught',\n",
       " 'caus',\n",
       " 'center',\n",
       " 'chanc',\n",
       " 'chang',\n",
       " 'charg',\n",
       " 'check',\n",
       " 'chemic',\n",
       " 'child',\n",
       " 'children',\n",
       " 'china',\n",
       " 'christian',\n",
       " 'citi',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'cliff',\n",
       " 'climat',\n",
       " 'close',\n",
       " 'coach',\n",
       " 'collaps',\n",
       " 'collid',\n",
       " 'collis',\n",
       " 'come',\n",
       " 'comment',\n",
       " 'compani',\n",
       " 'complet',\n",
       " 'comput',\n",
       " 'confirm',\n",
       " 'content',\n",
       " 'continu',\n",
       " 'control',\n",
       " 'cool',\n",
       " 'costlier',\n",
       " 'could',\n",
       " 'counti',\n",
       " 'countri',\n",
       " 'cours',\n",
       " 'cover',\n",
       " 'crash',\n",
       " 'crazi',\n",
       " 'creat',\n",
       " 'crew',\n",
       " 'cri',\n",
       " 'crisi',\n",
       " 'cross',\n",
       " 'crush',\n",
       " 'curfew',\n",
       " 'cut',\n",
       " 'cyclon',\n",
       " 'daili',\n",
       " 'damag',\n",
       " 'damn',\n",
       " 'danger',\n",
       " 'data',\n",
       " 'day',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'debri',\n",
       " 'declar',\n",
       " 'deliv',\n",
       " 'delug',\n",
       " 'demolish',\n",
       " 'demolit',\n",
       " 'derail',\n",
       " 'desol',\n",
       " 'destroy',\n",
       " 'destruct',\n",
       " 'deton',\n",
       " 'devast',\n",
       " 'die',\n",
       " 'disast',\n",
       " 'displac',\n",
       " 'dog',\n",
       " 'done',\n",
       " 'drake',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'drought',\n",
       " 'drown',\n",
       " 'due',\n",
       " 'dust',\n",
       " 'earthquak',\n",
       " 'east',\n",
       " 'ebay',\n",
       " 'ebola',\n",
       " 'effect',\n",
       " 'electrocut',\n",
       " 'els',\n",
       " 'emerg',\n",
       " 'emot',\n",
       " 'end',\n",
       " 'engulf',\n",
       " 'escap',\n",
       " 'evacu',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'everi',\n",
       " 'everyon',\n",
       " 'everyth',\n",
       " 'exchang',\n",
       " 'expect',\n",
       " 'expert',\n",
       " 'explod',\n",
       " 'explos',\n",
       " 'eye',\n",
       " 'eyewit',\n",
       " 'face',\n",
       " 'failur',\n",
       " 'fall',\n",
       " 'famili',\n",
       " 'famin',\n",
       " 'fan',\n",
       " 'fatal',\n",
       " 'favorit',\n",
       " 'fear',\n",
       " 'fedex',\n",
       " 'feel',\n",
       " 'fight',\n",
       " 'final',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'firefight',\n",
       " 'first',\n",
       " 'flag',\n",
       " 'flame',\n",
       " 'flash',\n",
       " 'flatten',\n",
       " 'flood',\n",
       " 'follow',\n",
       " 'food',\n",
       " 'forc',\n",
       " 'forest',\n",
       " 'found',\n",
       " 'free',\n",
       " 'friend',\n",
       " 'fuck',\n",
       " 'fukushima',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'gener',\n",
       " 'germ',\n",
       " 'get',\n",
       " 'giant',\n",
       " 'girl',\n",
       " 'give',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goe',\n",
       " 'gon',\n",
       " 'good',\n",
       " 'gop',\n",
       " 'got',\n",
       " 'govern',\n",
       " 'great',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'gt',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'haha',\n",
       " 'hail',\n",
       " 'half',\n",
       " 'handbag',\n",
       " 'happen',\n",
       " 'happi',\n",
       " 'harm',\n",
       " 'hate',\n",
       " 'hazard',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heavi',\n",
       " 'helicopt',\n",
       " 'hell',\n",
       " 'hellfir',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'high',\n",
       " 'hijack',\n",
       " 'hire',\n",
       " 'hiroshima',\n",
       " 'histori',\n",
       " 'hit',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'horribl',\n",
       " 'horror',\n",
       " 'hors',\n",
       " 'hostag',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hous',\n",
       " 'http',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'hundr',\n",
       " 'hurrican',\n",
       " 'im',\n",
       " 'imag',\n",
       " 'india',\n",
       " 'injur',\n",
       " 'injuri',\n",
       " 'insid',\n",
       " 'insur',\n",
       " 'interest',\n",
       " 'intern',\n",
       " 'inund',\n",
       " 'investig',\n",
       " 'iran',\n",
       " 'isi',\n",
       " 'islam',\n",
       " 'island',\n",
       " 'isra',\n",
       " 'issu',\n",
       " 'japan',\n",
       " 'job',\n",
       " 'keep',\n",
       " 'kid',\n",
       " 'kill',\n",
       " 'king',\n",
       " 'knock',\n",
       " 'know',\n",
       " 'la',\n",
       " 'lab',\n",
       " 'ladi',\n",
       " 'lake',\n",
       " 'land',\n",
       " 'landslid',\n",
       " 'last',\n",
       " 'latest',\n",
       " 'lava',\n",
       " 'lead',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leather',\n",
       " 'leav',\n",
       " 'left',\n",
       " 'legionnair',\n",
       " 'let',\n",
       " 'level',\n",
       " 'libya',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'light',\n",
       " 'lightn',\n",
       " 'like',\n",
       " 'line',\n",
       " 'link',\n",
       " 'listen',\n",
       " 'liter',\n",
       " 'littl',\n",
       " 'live',\n",
       " 'lmao',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'low',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'made',\n",
       " 'major',\n",
       " 'make',\n",
       " 'malaysia',\n",
       " 'man',\n",
       " 'mani',\n",
       " 'manslaught',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'mass',\n",
       " 'massacr',\n",
       " 'may',\n",
       " 'mayb',\n",
       " 'mean',\n",
       " 'media',\n",
       " 'meek',\n",
       " 'meltdown',\n",
       " 'member',\n",
       " 'memori',\n",
       " 'men',\n",
       " 'mh370',\n",
       " 'might',\n",
       " 'migrant',\n",
       " 'militari',\n",
       " 'million',\n",
       " 'minut',\n",
       " 'mishap',\n",
       " 'miss',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'morn',\n",
       " 'mosqu',\n",
       " 'mountain',\n",
       " 'move',\n",
       " 'movi',\n",
       " 'mp',\n",
       " 'mph',\n",
       " 'much',\n",
       " 'mudslid',\n",
       " 'murder',\n",
       " 'music',\n",
       " 'muslim',\n",
       " 'must',\n",
       " 'na',\n",
       " 'name',\n",
       " 'nation',\n",
       " 'natur',\n",
       " 'near',\n",
       " 'nearbi',\n",
       " 'nearli',\n",
       " 'need',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nigga',\n",
       " 'night',\n",
       " 'north',\n",
       " 'northern',\n",
       " 'noth',\n",
       " 'nowplay',\n",
       " 'nuclear',\n",
       " 'number',\n",
       " 'nw',\n",
       " 'obama',\n",
       " 'obliter',\n",
       " 'offens',\n",
       " 'offic',\n",
       " 'offici',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'one',\n",
       " 'onlin',\n",
       " 'open',\n",
       " 'order',\n",
       " 'other',\n",
       " 'outbreak',\n",
       " 'outrag',\n",
       " 'outsid',\n",
       " 'pakistan',\n",
       " 'pandemonium',\n",
       " 'panic',\n",
       " 'panick',\n",
       " 'park',\n",
       " 'part',\n",
       " 'pass',\n",
       " 'passeng',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'peac',\n",
       " 'peopl',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'photo',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'pkk',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'plane',\n",
       " 'play',\n",
       " 'pleas',\n",
       " 'pm',\n",
       " 'point',\n",
       " 'polic',\n",
       " 'polici',\n",
       " 'poor',\n",
       " 'possibl',\n",
       " 'post',\n",
       " 'power',\n",
       " 'pray',\n",
       " 'prebreak',\n",
       " 'prepar',\n",
       " 'pretti',\n",
       " 'probabl',\n",
       " 'project',\n",
       " 'public',\n",
       " 'put',\n",
       " 'quarantin',\n",
       " 'rain',\n",
       " 'rainstorm',\n",
       " 'raze',\n",
       " 'rd',\n",
       " 'reactor',\n",
       " 'read',\n",
       " 'readi',\n",
       " 'real',\n",
       " 'realli',\n",
       " 'reason',\n",
       " 'red',\n",
       " 'reddit',\n",
       " 'refuge',\n",
       " 'refugio',\n",
       " 'releas',\n",
       " 'rememb',\n",
       " 'report',\n",
       " 'rescu',\n",
       " 'rescuer',\n",
       " 'respond',\n",
       " 'result',\n",
       " 'reunion',\n",
       " 'reuter',\n",
       " 'review',\n",
       " 're\\x89û_',\n",
       " 'right',\n",
       " 'riot',\n",
       " 'rise',\n",
       " 'risk',\n",
       " 'river',\n",
       " 'road',\n",
       " 'rock',\n",
       " 'rt',\n",
       " 'rubbl',\n",
       " 'ruin',\n",
       " 'rule',\n",
       " 'run',\n",
       " 'russian',\n",
       " 'said',\n",
       " 'saipan',\n",
       " 'sandstorm',\n",
       " 'saudi',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'scare',\n",
       " 'school',\n",
       " 'scream',\n",
       " 'search',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'secur',\n",
       " 'see',\n",
       " 'seek',\n",
       " 'seen',\n",
       " 'seismic',\n",
       " 'self',\n",
       " 'send',\n",
       " 'servic',\n",
       " 'set',\n",
       " 'sever',\n",
       " 'share',\n",
       " 'ship',\n",
       " 'shit',\n",
       " 'shoot',\n",
       " 'shot',\n",
       " 'shoulder',\n",
       " 'show',\n",
       " 'side',\n",
       " 'sign',\n",
       " 'sinc',\n",
       " 'sink',\n",
       " 'sinkhol',\n",
       " 'siren',\n",
       " 'site',\n",
       " 'sky',\n",
       " 'smoke',\n",
       " 'snowstorm',\n",
       " 'someon',\n",
       " 'someth',\n",
       " 'song',\n",
       " 'soon',\n",
       " 'soudelor',\n",
       " 'sound',\n",
       " 'south',\n",
       " 'space',\n",
       " 'special',\n",
       " 'spill',\n",
       " 'spot',\n",
       " 'st',\n",
       " 'stand',\n",
       " 'star',\n",
       " 'start',\n",
       " 'state',\n",
       " 'stay',\n",
       " 'still',\n",
       " 'stock',\n",
       " 'stop',\n",
       " 'stori',\n",
       " 'storm',\n",
       " 'street',\n",
       " 'stretcher',\n",
       " 'strike',\n",
       " 'structur',\n",
       " 'sue',\n",
       " 'suicid',\n",
       " 'summer',\n",
       " 'sunk',\n",
       " 'support',\n",
       " 'sure',\n",
       " 'surviv',\n",
       " 'survivor',\n",
       " 'suspect',\n",
       " 'swallow',\n",
       " 'take',\n",
       " 'talk',\n",
       " 'target',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'terror',\n",
       " 'terrorist',\n",
       " 'test',\n",
       " 'texa',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thousand',\n",
       " 'three',\n",
       " 'thunder',\n",
       " 'thunderstorm',\n",
       " 'time',\n",
       " 'today',\n",
       " 'tomorrow',\n",
       " 'tonight',\n",
       " 'top',\n",
       " 'tornado',\n",
       " 'total',\n",
       " 'tote',\n",
       " 'town',\n",
       " 'track',\n",
       " 'traffic',\n",
       " 'tragedi',\n",
       " 'train',\n",
       " 'transport',\n",
       " 'trap',\n",
       " 'trauma',\n",
       " 'traumatis',\n",
       " 'tree',\n",
       " 'trench',\n",
       " 'tri',\n",
       " 'troubl',\n",
       " 'truck',\n",
       " 'tsunami',\n",
       " 'turkey',\n",
       " 'turn',\n",
       " 'tv',\n",
       " 'tweet',\n",
       " 'twister',\n",
       " 'twitter',\n",
       " 'two',\n",
       " 'typhoon',\n",
       " 'typhoon-devast',\n",
       " 'u',\n",
       " 'u.s.',\n",
       " 'unit',\n",
       " 'updat',\n",
       " 'upheav',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'usa',\n",
       " 'use',\n",
       " 'via',\n",
       " 'victim',\n",
       " 'video',\n",
       " 'view',\n",
       " 'villag',\n",
       " 'violent',\n",
       " 'volcano',\n",
       " 'vote',\n",
       " 'w/',\n",
       " 'wait',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'want',\n",
       " 'war',\n",
       " 'warn',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'wave',\n",
       " 'way',\n",
       " 'weapon',\n",
       " 'weather',\n",
       " 'week',\n",
       " 'well',\n",
       " 'went',\n",
       " 'west',\n",
       " 'whirlwind',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'wild',\n",
       " 'wildfir',\n",
       " 'win',\n",
       " 'wind',\n",
       " 'windstorm',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'wonder',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'worri',\n",
       " 'would',\n",
       " 'wound',\n",
       " 'wreck',\n",
       " 'wreckag',\n",
       " 'wrong',\n",
       " 'ye',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'yet',\n",
       " 'york',\n",
       " 'youtub',\n",
       " 'zone'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get rid of noise, keep top 10% words\n",
    "\n",
    "wordfreq = {}\n",
    "for sentence in X_train:\n",
    "    for word in sentence:\n",
    "        if word not in wordfreq:\n",
    "            wordfreq[word] = 1\n",
    "        else:\n",
    "            wordfreq[word] += 1\n",
    "\n",
    "# experiment and play with proportions until we see a good collections of words \n",
    "most_freq = heapq.nlargest(round(0.05*len(wordfreq)), wordfreq, key=wordfreq.get)\n",
    "set_most_freq = set(most_freq)\n",
    "# get rid of useless_words\n",
    "set_useless_words = set([\"''\",\"'d\",  \"'m\",  \"'s\", \"'ve\", '--', '...', '1', '10','\\x96','2','3','5','``',\n",
    "                        \"\\x89û_\",\"'s\", '1', '2','2015','3','4','5','70','16yr','\\x89ûó','\\x89ûò', \"n't\"\n",
    "                        ])\n",
    "set_most_freq = set_most_freq.difference(set_useless_words)\n",
    "display(set_most_freq)\n",
    "\n",
    "# keep only 10 % words\n",
    "X_train = X_train.apply(lambda x: [i for i in x if i in set_most_freq])\n",
    "X_test = X_test.apply(lambda x: [i for i in x if i in set_most_freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log regression (bag of words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pass</th>\n",
       "      <th>cat</th>\n",
       "      <th>today</th>\n",
       "      <th>trench</th>\n",
       "      <th>villag</th>\n",
       "      <th>bestnaijamad</th>\n",
       "      <th>fan</th>\n",
       "      <th>ur</th>\n",
       "      <th>first</th>\n",
       "      <th>50</th>\n",
       "      <th>...</th>\n",
       "      <th>old</th>\n",
       "      <th>realli</th>\n",
       "      <th>music</th>\n",
       "      <th>intern</th>\n",
       "      <th>coach</th>\n",
       "      <th>failur</th>\n",
       "      <th>like</th>\n",
       "      <th>answer</th>\n",
       "      <th>movi</th>\n",
       "      <th>deton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pass  cat  today  trench  villag  bestnaijamad  fan  ur  first  50  ...  \\\n",
       "0     0    0      0       0       0             0    0   0      0   0  ...   \n",
       "1     0    0      0       0       0             0    0   0      0   0  ...   \n",
       "\n",
       "   old  realli  music  intern  coach  failur  like  answer  movi  deton  \n",
       "0    0       0      0       0      0       0     0       0     0      0  \n",
       "1    0       0      0       0      0       0     0       0     0      0  \n",
       "\n",
       "[2 rows x 746 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pass</th>\n",
       "      <th>cat</th>\n",
       "      <th>today</th>\n",
       "      <th>trench</th>\n",
       "      <th>villag</th>\n",
       "      <th>bestnaijamad</th>\n",
       "      <th>fan</th>\n",
       "      <th>ur</th>\n",
       "      <th>first</th>\n",
       "      <th>50</th>\n",
       "      <th>...</th>\n",
       "      <th>old</th>\n",
       "      <th>realli</th>\n",
       "      <th>music</th>\n",
       "      <th>intern</th>\n",
       "      <th>coach</th>\n",
       "      <th>failur</th>\n",
       "      <th>like</th>\n",
       "      <th>answer</th>\n",
       "      <th>movi</th>\n",
       "      <th>deton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pass  cat  today  trench  villag  bestnaijamad  fan  ur  first  50  ...  \\\n",
       "0     0    0      0       0       0             0    0   0      0   0  ...   \n",
       "1     0    0      0       0       0             0    0   0      0   0  ...   \n",
       "\n",
       "   old  realli  music  intern  coach  failur  like  answer  movi  deton  \n",
       "0    0       0      0       0      0       0     0       0     0      0  \n",
       "1    0       0      0       0      0       0     0       0     0      0  \n",
       "\n",
       "[2 rows x 746 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the columns\n",
    "\n",
    "def make_columns(X, set_freq):\n",
    "    word_dict = {}\n",
    "    for token in set_freq:\n",
    "        word_list = []\n",
    "        for sentence in X:\n",
    "            if token in sentence:\n",
    "                word_list.append(1)\n",
    "            else:\n",
    "                word_list.append(0)\n",
    "        word_dict[token] = word_list\n",
    "    return word_dict\n",
    "\n",
    "X_train_word_dict = make_columns(X_train, set_most_freq)\n",
    "X_test_word_dict = make_columns(X_test, set_most_freq)\n",
    "\n",
    "# Build the bag of words matrix\n",
    "X_train_processed = pd.DataFrame(X_train_word_dict)\n",
    "display(X_train_processed.head(2))\n",
    "\n",
    "column_order = X_train_processed.columns\n",
    "\n",
    "X_test_processed = pd.DataFrame(X_test_word_dict)\n",
    "X_test_processed = X_test_processed[column_order] # make sure same column order\n",
    "display(X_test_processed.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8394117647058823\n",
      "0.7954635893354556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_processed)\n",
    "y_pred_test  = clf.predict(X_test_processed)\n",
    "\n",
    "print(accuracy_score(y_pred_train, y_train))\n",
    "print(accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "# ensure training score not too far away  from testing. Otherwise, it may be an indication of over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Manual inspection, to see if it is a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2909\n",
      "1    2191\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\waish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>original_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>8578</td>\n",
       "      <td>screams</td>\n",
       "      <td>Sheffield/Leeds</td>\n",
       "      <td>[agre, certain, cultur, appropri, thing, hones...</td>\n",
       "      <td>1</td>\n",
       "      <td>I agree with certain cultural appropriation th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>6655</td>\n",
       "      <td>landslide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[hoodedu, fuck, better, berlatski, n't, win, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>@hoodedu You fucking better Berlatsky.  If I d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>4395</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>[sure, megaquak, stori, brought, sens, panic, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sure the #Megaquake story brought a sense of p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>4416</td>\n",
       "      <td>electrocute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[danisnotonfir, n't, let, phil, help, 'll, pro...</td>\n",
       "      <td>0</td>\n",
       "      <td>@danisnotonfire don't let Phil help out he'll ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword         location  \\\n",
       "6006  8578      screams  Sheffield/Leeds   \n",
       "4681  6655    landslide              NaN   \n",
       "3063  4395   earthquake      Seattle, WA   \n",
       "3079  4416  electrocute              NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "6006  [agre, certain, cultur, appropri, thing, hones...       1   \n",
       "4681  [hoodedu, fuck, better, berlatski, n't, win, f...       0   \n",
       "3063  [sure, megaquak, stori, brought, sens, panic, ...       1   \n",
       "3079  [danisnotonfir, n't, let, phil, help, 'll, pro...       0   \n",
       "\n",
       "                                          original_text  prediction  \n",
       "6006  I agree with certain cultural appropriation th...           0  \n",
       "4681  @hoodedu You fucking better Berlatsky.  If I d...           0  \n",
       "3063  Sure the #Megaquake story brought a sense of p...           0  \n",
       "3079  @danisnotonfire don't let Phil help out he'll ...           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# is this a good model\n",
    "# The split is about 50/50, so a 82% accuracy model isnt bad!\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# check answers\n",
    "check_answers = df.iloc[y_test.index,:]\n",
    "check_answers['prediction'] = y_pred_test\n",
    "display(check_answers.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL :  1 PREDICTION :  0\n",
      "I agree with certain cultural appropriation things but honestly if u looked at my house it screams appropriation bc Buddhas and stuff-\n",
      "#############################\n",
      "ACTUAL :  1 PREDICTION :  0\n",
      "Sure the #Megaquake story brought a sense of panic but the question is: will anything really change? http://t.co/9f3rDN9N3D\n",
      "#############################\n",
      "ACTUAL :  1 PREDICTION :  0\n",
      "Learning from the Legacy of a Catastrophic Eruption http://t.co/25sY9Y295L via @newyorker\n",
      "#############################\n",
      "ACTUAL :  1 PREDICTION :  0\n",
      "Telnet attacked from 124.13.172.40 (STREAMYX-HOME-SOUTHERN MY)\n",
      "#############################\n",
      "ACTUAL :  1 PREDICTION :  0\n",
      "Zayn Malik &amp; Perrie Edwards End Engagement: SheÛªs Û÷DevastatedÛª http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD\n",
      "#############################\n"
     ]
    }
   ],
   "source": [
    "# check what's wrong\n",
    "check_wrong = check_answers[check_answers['target'] != check_answers['prediction']].reset_index()\n",
    "for i in range(5):\n",
    "    print('ACTUAL : ', check_wrong['target'][i], 'PREDICTION : ', check_wrong['prediction'][i])\n",
    "    print(check_wrong['original_text'][i])\n",
    "    print('#############################')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To be fair, I would have guessed some of these tweets wrong too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "There are some errors in the training data. We can see that Zayn Malik is marked as 1, but Zayn is definitely not related to the weathe catastrphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Zayn just blew up twitter.\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Zayn Malik &amp; Perrie Edwards End Engagement: SheÛªs Û÷DevastatedÛª http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Me pulling over and fighting the hoes that called Zayn a terrorist  http://t.co/FY30fV0Qbx\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df['original_text'] = df['text'].copy()\n",
    "\n",
    "\n",
    "zayn = df[df['original_text'].str.contains('Zayn')].reset_index()\n",
    "for i in range(3):\n",
    "    print(zayn['target'][i])\n",
    "    print(zayn['original_text'][i])\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
